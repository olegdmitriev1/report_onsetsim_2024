---
output:
  pdf_document:
    fig_caption: no
    number_sections: no
    toc: yes
    toc_depth: 2
    # github_document:
    # html_preview: yes
    # toc: yes
    # toc_depth: 2
editor_options: 
  chunk_output_type: inline
---

Here I varied the sample size of trials from 20 to 150 in increments of 10 based on the code from Rousselet (2023). 

# Dependencies
```{r setup, message=FALSE}
library(ggplot2)
library(tibble)
library(cowplot)
library(beepr)
library(Rfast)
library(changepoint)
library(mcp)
library(rjags)
library(mutoss)
library(ecp)
source("./code/functions.R")
source("./code/theme_gar.txt")
# Edit `one_over_f` function from `primer` package to control variance (Stevens, 2009). 
# Original function is available on [GitHub](https://github.com/HankStevens/primer).
# Copyright Hank Stevens.
source("./code/one_over_f.R")
# Load template: true onset = 160 ms, F=81, max at F=126
source("./code/erp_template.R")
# R version of Matlab code from Yeung et al. 2004
source("./code/eeg_noise.R")
# Lodaing the code that gives median instead of mean values in mcp summary table
source("./code/mcp_median_function.R")
# to use with eeg_noise function
meanpower <- unlist(read.table("./code/meanpower.txt"))
```

# Simulation: vary sample size

The following simulation was broken apart to run each method individually with a seed of 666. The results were merged later in this file into a single graph with all methods. Original code for individual simulations can be found in the vary_n_code folder.
```{r eval=FALSE, message=TRUE, warning=FALSE, include=FALSE}

# Saving function to save simulation results incrementally every 100 simulation iterations
saving_results <- function(iteration, simres.cs, simres.fdr, simres.cp, simres.pelt, simres.mcp, simres.ecp) {
  filename <- sprintf("./vary_n_sim_incr_save_data/main_vary_n_iter_%d.RData", iteration)
  save(simres.cs, simres.fdr, simres.cp, simres.pelt, simres.mcp, simres.ecp, file = filename)
}

set.seed(666) 
aath <- 0.05 # arbitrary alpha threshold
nsim <- 10000 # simulation iterations
nboot <- 2 # number of permutation samples
srate <- 500 # sampling rate in Hz
n_vec <- seq(20,150,10) 
n_length <- length(n_vec)
n_max <- max(n_vec)
inc.step <- 500

# Matrices for each method
simres.cp <- matrix(NA, nrow = n_length, ncol = nsim)
simres.fdr <- matrix(NA, nrow = n_length, ncol = nsim)
simres.cs <- matrix(NA, nrow = n_length, ncol = nsim)
simres.mcp <- matrix(NA, nrow = n_length, ncol = nsim)
simres.pelt <- matrix(NA, nrow = n_length, ncol = nsim)
simres.ecp <- matrix(NA, nrow = n_length, ncol = nsim)
simres.mcp.mean <- matrix(NA, nrow = n_length, ncol = nsim)

outvar <- 1 # noise variance
cond1_all <- matrix(0, nrow = n_max, ncol = Nf)
cond2_all <- matrix(0, nrow = n_max, ncol = Nf)

for(S in 1:nsim){
  
  sim.counter(S, nsim, inc = inc.step)
  
  # Generate all trials
  for(T in 1:n_max){
    cond2_all[T,] <- temp2 + eeg_noise(frames = Nf, srate = srate, outvar = outvar, meanpower)
    cond1_all[T,] <- temp1 + eeg_noise(frames = Nf, srate = srate, outvar = outvar, meanpower) 
  }
  
  for(N in 1:n_length){
    
    Nt <- n_vec[N]
    
    # downsample to current size
    cond2 <- cond2_all[1:Nt,]
    cond1 <- cond1_all[1:Nt,]
    
    # t-tests
    ori.t2 <- vector(mode = "numeric", length = Nf)
    for(F in 1:Nf){
      ori.t2[F] <- t.test(cond1[,F], cond2[,F])$statistic^2
    }
    
    #######################################
    
    # fit change point model
    res <- cpt.meanvar(ori.t2, method = "BinSeg", Q=2)
    simres.cp[N,S] <- Xf[res@cpts[1]]
    
    #######################################
    
    # Fit PELT with a penalty multiplier of 30
    res.pelt <- cpt.meanvar(ori.t2, method = "PELT", penalty = "Manual", pen.value = 30*log(length(ori.t2)))
    simres.pelt[N,S] <- Xf[res.pelt@cpts[1]]
    
    #######################################
    
    # Fit mcp model
    df <- tibble(x = Xf, y = ori.t2)
    model <- list(
      y ~ 1 + sigma(1),
        ~ 0 + x + sigma(1)
    )
    
    prior <- list(
      cp_1 = "dunif(100, 250)" # Change point expected between 100 and 250 ms
    )
  
    fit <- mcp(model, prior = prior, data = df, cores = 3, chains = 3)
  
    summary.fit.median <- summary.mcpfit.median(fit)
    simres.mcp[N,S] <- round(summary.fit.median[1,2],0)
  
    # Fitting the same model but for the mean function to compare to median
    summary.fit.mean <- mcp:::summary.mcpfit(fit)
    simres.mcp.mean[N,S] <- round(summary.fit.mean[1,2],0)
  
    ######################################
    
    # Make permutation table of t values 
    perm.t2 <- permtdist(cond1, cond2, Nt, Nf, nboot = nboot)^2
    perm.th <- apply(perm.t2, 2, quantile, probs = 1-aath)
    
    # FDR -----
    perm.pvals <- vector(mode = "numeric", length = Nf)
    for(F in 1:Nf){
      perm.pvals[F] <- (sum(perm.t2[,F] >= ori.t2[F]) + 1) / (nboot + 1)
    }
    fdr.pvals <- p.adjust(perm.pvals, method = "fdr")
    simres.fdr[N,S] <- find_onset(fdr.pvals <= aath, Xf)
    
    ######################################
    
    # cluster-sum statistics -----
    cmap <- cluster.make(perm.pvals <= aath)
    perm.max.sums <- vector(mode = "numeric", length = nboot)
    for(B in 1:nboot){
      # threshold permutation t2 values and form clusters
      perm.cmap <- cluster.make(perm.t2[B,] <= perm.th)  
      perm.max.sums[B] <- max(cluster.sum(values = perm.t2[B,], cmap = perm.cmap))
    }
    # cluster sum threshold
    cs.th <- quantile(perm.max.sums, probs = 1-aath)
    # cluster test
    cs.test <- cluster.test(values = ori.t2, cmap = cmap, cs.th)
    simres.cs[N,S] <- find_onset(cs.test, Xf)
    
    #######################################
    
    # Fit non-parametric e.cp3o_delta from ecp package
    ori.t2 <- matrix(ori.t2, ncol = 1) # ecp only accepts matrices 
    
    # Fit ecp cp3o_delta model
    result_cp3o_delta <- e.cp3o_delta(Z = ori.t2, K = 8, alpha = 1)
    simres.ecp[N,S] <- get_earliest_cp(result_cp3o_delta$estimates, Xf)
  }
  
  # incremental results save
  if (S %% 500 == 0 || S == nsim) {
    saving_results(S, simres.cs, simres.fdr, simres.cp, simres.pelt, simres.mcp, simres.ecp)
  }
}
```


## Load in data
```{r}
# BinSeg data
load("./data/onsetsim_varyn_eegnoise.RData")
simres.bs <- simres.cp
rm(simres.cp,simres.max)
load("./data/vary_n_pelt.RData")
load("./data/vary_n_bcp.RData")
load("./data/vary_n_cp3o.RData")
load("./data/vary_n_bcp.RData")
load("./data/vary_n_mcp.RData")

# # Loading and merging mcp data
# load("./vary_n_mcp_data/vary_n_mcp_2000.RData")
# load("./vary_n_mcp_data4/vary_n_mcp_2000.RData") # loads in as temp.simres2
# temp.simres4 <- temp.simres2
# load("./vary_n_mcp_data2/vary_n_mcp_2200.RData")
# load("./vary_n_mcp_data3/vary_n_mcp_2000.RData")
# load("./vary_n_mcp_data5/vary_n_mcp_700.RData")
# load("./vary_n_mcp_data6/vary_n_mcp_700.RData")
# load("./vary_n_mcp_data7/vary_n_mcp_400.RData")
# 
# simres.mcp <- cbind(temp.simres, temp.simres2, temp.simres3, temp.simres4, temp.simres5, temp.simres6, temp.simres7)
```

## Results

Plot results as a function of sample size.

### Compute summary statistics
```{r}
# load(file = "./vary_n_sim_incr_save_data/main_sim_vary_n.RData")

n_vec <- seq(20,150,10) 
n_length <- length(n_vec)

res.bias <- matrix(0, nrow = 7, ncol = n_length) 
res.mae <- matrix(0, nrow = 7, ncol = n_length)
res.var <- matrix(0, nrow = 7, ncol = n_length)
res.pte <- matrix(0, nrow = 7, ncol = n_length)
# res.p40 <- matrix(0, nrow = 7, ncol = n_length)

for(N in 1:n_length){
  #Bias
  res.bias[1,N] <- median(simres.fdr[N,], na.rm = TRUE) - true_onset
  res.bias[2,N] <- median(simres.cs[N,], na.rm = TRUE) - true_onset
  res.bias[3,N] <- median(simres.bs[N,], na.rm = TRUE) - true_onset
  res.bias[4,N] <- median(simres.mcp[N,], na.rm = TRUE) - true_onset
  res.bias[5,N] <- median(simres.cp3o[N,], na.rm = TRUE) - true_onset
  res.bias[6,N] <- median(simres.pelt[N,], na.rm = TRUE) - true_onset
  res.bias[7,N] <- median(simres.bcp[N,], na.rm = TRUE) - true_onset
  
  #Mean absolute error 
  res.mae[1,N] <- mean(abs(simres.fdr[N,] - true_onset), na.rm = TRUE)
  res.mae[2,N] <- mean(abs(simres.cs[N,] - true_onset), na.rm = TRUE)
  res.mae[3,N] <- mean(abs(simres.bs[N,] - true_onset), na.rm = TRUE)
  res.mae[4,N] <- mean(abs(simres.mcp[N,] - true_onset), na.rm = TRUE)
  res.mae[5,N] <- mean(abs(simres.cp3o[N,] - true_onset), na.rm = TRUE)
  res.mae[6,N] <- mean(abs(simres.pelt[N,] - true_onset), na.rm = TRUE)
  res.mae[7,N] <- mean(abs(simres.bcp[N,] - true_onset), na.rm = TRUE)
  
  #Variance
  res.var[1,N] <- var(simres.fdr[N,], na.rm = TRUE)
  res.var[2,N] <- var(simres.cs[N,], na.rm = TRUE)
  res.var[3,N] <- var(simres.bs[N,], na.rm = TRUE)
  res.var[4,N] <- var(simres.mcp[N,], na.rm = TRUE)
  res.var[5,N] <- var(simres.cp3o[N,], na.rm = TRUE)
  res.var[6,N] <- var(simres.pelt[N,], na.rm = TRUE)
  res.var[7,N] <- var(simres.bcp[N,], na.rm = TRUE)
  
  #Proportion too early
  res.pte[1,N] <- mean((simres.fdr[N,] - true_onset) < 0, na.rm = TRUE)
  res.pte[2,N] <- mean((simres.cs[N,] - true_onset) < 0, na.rm = TRUE)
  res.pte[3,N] <- mean((simres.bs[N,] - true_onset) < 0, na.rm = TRUE)
  res.pte[4,N] <- mean((simres.mcp[N,] - true_onset) < 0, na.rm = TRUE)
  res.pte[5,N] <- mean((simres.cp3o[N,] - true_onset) < 0, na.rm = TRUE)
  res.pte[6,N] <- mean((simres.pelt[N,] - true_onset) < 0, na.rm = TRUE)
  res.pte[7,N] <- mean((simres.bcp[N,] - true_onset) < 0, na.rm = TRUE)
}
```

### Make figures 
#### Bias
```{r fig.height=6, fig.width=8}
# load(file = "./data/onsetsim_varyn_eegnoise.RData")
# load("./data/vary_n_bcp.RData")

# Colour palette from http://www.cookbook-r.com/Graphs/Colors_(ggplot2)/
categ.palette <- c("FDR" = "#D55E00",
                   "CS" = "#0072B2",
                   "BinSeg" = "#E69F00",
                   "mcp" = "#CC79A7",
                   "cp3o_delta" = "#009E73",
                   "PELT" = "#F0E442",
                   "bcp" = "#000000")


df <- tibble(res = as.vector(res.bias),
             n = rep(n_vec, each = 7),
             method = rep(c("FDR", "CS", "BinSeg", "mcp", "cp3o_delta", "PELT", "bcp"), n_length)
)

p.bias <- ggplot(df, aes(x = n, y = res, group = method, colour = method)) + theme_gar +
  geom_point(show.legend = F) +
  geom_line(show.legend = F) +
  scale_colour_manual(values = categ.palette) +
  labs(x = "Sample size", y = "Bias") +
  scale_x_continuous(breaks = seq(20,150,20)) +
  scale_y_continuous(breaks = seq(-40,70,10), limits = c(-40, 70))


# ggsave(filename = "./figures/eeg_varyn_bias.pdf", width = 10, height = 5)
```

#### MAE
```{r fig.height=6, fig.width=8}
categ.palette <- c("FDR" = "#D55E00",
                   "CS" = "#0072B2",
                   "BinSeg" = "#E69F00",
                   "mcp" = "#CC79A7",
                   "cp3o_delta" = "#009E73",
                   "PELT" = "#F0E442",
                   "bcp" = "#000000")

df <- tibble(res = as.vector(res.mae),
             n = rep(n_vec, each = 7),
             method = rep(c("FDR", "Cluster sum", "BinSeg", "mcp", "cp3o_delta", "PELT", "bcp"), n_length)
)


p.mae <- ggplot(df, aes(x = n, y = res, group = method, colour = method)) + theme_gar +
  geom_point(show.legend = F) +
  geom_line(show.legend = F) +
  scale_colour_manual(values = categ.palette) +
  labs(x = "Sample size", y = "MAE") +
  scale_x_continuous(breaks = seq(20,150,20)) + theme(legend.position = "none")
p.mae
```

#### Variance
```{r fig.height=6, fig.width=8}
df <- tibble(res = as.vector(res.var),
             n = rep(n_vec, each = 7),
             method = rep(c("FDR", "CS", "BinSeg", "mcp", "cp3o_delta", "PELT", "bcp"), n_length)
)

p.var <- ggplot(df, aes(x = n, y = res, group = method, colour = method)) + theme_gar +
  geom_point() +
  geom_line() +
  scale_colour_manual(values = categ.palette) +
  labs(x = "Sample size", y = "Variance") +
  theme(legend.position = c(.8, .7), legend.text = element_text(size = 10), legend.title = element_text(size = 11), legend.key.size = unit(0.5, "cm"),
  legend.spacing.y = unit(0.1, "cm")) +
  scale_x_continuous(breaks = seq(20,150,20)) +
  scale_y_continuous(breaks = seq(0,13000,3000))
  # scale_y_continuous(breaks = seq(0,70,10))
p.var

# ggsave(filename = "./figures/eeg_varyn_var.pdf", width = 10, height = 5)
```


#### Proportion too early

```{r fig.height=6, fig.width=8}

df <- tibble(res = as.vector(res.pte),
             n = rep(n_vec, each = 7),
             method = rep(c("FDR", "CS", "BinSeg", "mcp", "cp3o_delta", "PELT", "bcp"), n_length)
)

p.pte <- ggplot(df, aes(x = n, y = res, group = method, colour = method)) + theme_gar +
  geom_point(show.legend = FALSE) +
  geom_line(show.legend = FALSE) +
  scale_colour_manual(values = categ.palette) +
  labs(x = "Sample size", y = "Proportion too early") +
  scale_x_continuous(breaks = seq(20,150,20))
```

```{r fig.width=12, fig.height=8}
cowplot::plot_grid(p.bias, p.mae, p.var, p.pte,
                   nrow = 2, 
                   labels = c("A", "B", "C", "D", "F"),
                   label_size = 15)

ggsave(filename = "./figures/vary_n_plot.pdf", width = 12, height = 8)
```

Rousselet GA (2023) Using cluster-based permutation tests to estimate MEG/EEG onsets: how bad is it? bioRxiv. https://doi.org/10.1101/2023.11.13.566864